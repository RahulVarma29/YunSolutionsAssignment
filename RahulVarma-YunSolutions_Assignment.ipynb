{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1ykIiglDoTqTTpFzQu5R36m1v9u7KdP3N","authorship_tag":"ABX9TyMp2cycKS9WGDpWM3HGSBm+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Assignment 1 - Speech Emotion Recognition"],"metadata":{"id":"a_ZDacQgNQFV"}},{"cell_type":"markdown","source":["In this assignment, I have used libraries **librosa, soundfile, glob, pickle and sklearn** to build a model using an **MLPClasssifier** and trained the model using **RAVDESS dataset** (Ryerson Audio-Visual Database of Emotional Speech and Song dataset) . We can also use any other pre-trained machine learning model that can classify emotions based on the extracted features. "],"metadata":{"id":"n80FA9unmj4N"}},{"cell_type":"markdown","source":["### Importing libraries"],"metadata":{"id":"cdivmUytqVto"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xk8-gwAq62Zw"},"outputs":[],"source":["import librosa #to extract the audio data from the MP3 file and perform speech processing techniques on it to identify the emotions\n","import librosa.display\n","import soundfile\n","import os, glob, pickle\n","import matplotlib.pyplot as plt #to plot a graph to visualize the emotional changes throughout the file\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"markdown","source":["### Extracting relevant features from sound file (mfcc, chroma, mel)"],"metadata":{"id":"djb2YM0qqddT"}},{"cell_type":"code","source":["# Extract features (mfcc, chroma, mel) from a sound file\n","def feature_extraction(audio_file, mfcc, chroma, mel):\n","  with soundfile.SoundFile(audio_file) as sound_file:\n","        X = sound_file.read(dtype=\"float32\")\n","        sample_rate=sound_file.samplerate\n","        if chroma:\n","            stft=np.abs(librosa.stft(X))\n","        result=np.array([])\n","        if mfcc:\n","            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n","            result=np.hstack((result, mfccs))\n","        if chroma:\n","            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n","            result=np.hstack((result, chroma))\n","        if mel:\n","            mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n","            result=np.hstack((result, mel))\n","  return result"],"metadata":{"id":"RKqcSDZmHgnT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Selecting emotions "],"metadata":{"id":"7qhTWkWQqu-Y"}},{"cell_type":"code","source":["# Emotions in the RAVDESS dataset\n","emotions={\n","  '01':'neutral',\n","  '02':'calm',\n","  '03':'happy',\n","  '04':'sad',\n","  '05':'angry',\n","  '06':'fearful',\n","  '07':'disgust',\n","  '08':'surprised'\n","}\n","# Emotions to observe\n","observed_emotions=[ 'happy', 'sad', 'angry','neutral']"],"metadata":{"id":"grzzyprzHgkZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Loading the data and extracting features for each sound file"],"metadata":{"id":"ld-2_egbsQSy"}},{"cell_type":"code","source":["def data_loading(test_size=0.2):\n","    x,y=[],[]\n","    for file in glob.glob(\"/content/drive/MyDrive/Colab Notebooks/Dataset\\\\Actor_*\\\\*.wav\"):\n","        file_name=os.path.basename(file)\n","        emotion=emotions[file_name.split(\"-\")[2]]\n","        if emotion not in observed_emotions:\n","            continue\n","        feature=feature_extraction(file, mfcc=True, chroma=True, mel=True)\n","        x.append(feature)\n","        y.append(emotion)\n","    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)"],"metadata":{"id":"kPze-XV4Hgho"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training and testing"],"metadata":{"id":"Yko0lOSgsWM2"}},{"cell_type":"code","source":["# Spliting the dataset into test and train\n","x_train,x_test,y_train,y_test=data_loading(test_size = 0.25)"],"metadata":{"id":"XWdFmSGfHgK5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# shape of the training and testing datasets\n","print((x_train.shape[0], x_test.shape[0]))"],"metadata":{"id":"sfyxnileHgGl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# number of features extracted\n","print(f'Features extracted: {x_train.shape[1]}')"],"metadata":{"id":"dAMuAtykHgE_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Building model"],"metadata":{"id":"DNFTRIp3slL_"}},{"cell_type":"code","source":["# Initialize the Multi Layer Perceptron Classifier\n","model=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)"],"metadata":{"id":"HuLfA3rsHgCa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","model.fit(x_train,y_train)"],"metadata":{"id":"8rKBkMwoHgAJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predict for the test set\n","y_pred=model.predict(x_test)"],"metadata":{"id":"IBE1cbCMHf9I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Accuracy"],"metadata":{"id":"nZeZ5Y2JouEf"}},{"cell_type":"code","source":["# Calculate the accuracy of our model\n","accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n","# Print the accuracy\n","print(\"Accuracy: {:.2f}%\".format(accuracy*100))"],"metadata":{"id":"ml4waNuAHf7G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Input audio file"],"metadata":{"id":"LQpO7s1HssxA"}},{"cell_type":"code","source":["# Load the audio file\n","audio_path = 'path/to/mp3/file.mp3'\n","y, sr = librosa.load(audio_path, duration=60)"],"metadata":{"id":"U5Fdea9f8inL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model prediction"],"metadata":{"id":"UuzvBIj6tDxJ"}},{"cell_type":"code","source":["# Extract features from the audio file\n","chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n","rmse = librosa.feature.rmse(y=y)\n","spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n","spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n","rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n","zcr = librosa.feature.zero_crossing_rate(y)"],"metadata":{"id":"aFd6OzPM8mrg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute the emotions from the features\n","features = np.vstack([chroma_stft, rmse, spec_cent, spec_bw, rolloff, zcr])\n","emotion_labels = ['happy', 'sad', 'angry', 'neutral']\n","emotion_predictions = model.predict(features.T)\n","predicted_emotions = [emotion_labels[np.argmax(pred)] for pred in emotion_predictions]"],"metadata":{"id":"KUO0ej6M8mYk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Plotting"],"metadata":{"id":"V66bkQszs42R"}},{"cell_type":"code","source":["# Plot the emotions as a function of time\n","plt.figure(figsize=(10, 4))\n","librosa.display.waveplot(y, sr=sr, alpha=0.5)\n","plt.plot(np.linspace(0, len(y) / sr, len(predicted_emotions)), predicted_emotions, color='r')\n","plt.title('Emotional Changes in the Audio File')\n","plt.xlabel('Time (s)')\n","plt.ylabel('Emotion')\n","plt.show()"],"metadata":{"id":"LTMmOBVg8mRj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Assignment 2 - Analysing Company Earning Calls"],"metadata":{"id":"ZwT9VsKMp20H"}},{"cell_type":"markdown","source":["### Importing libraries"],"metadata":{"id":"8PDngp9DnfCC"}},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import requests\n","\n","# For webscrapping purpose\n","from bs4 import BeautifulSoup"],"metadata":{"id":"pUxo4lnNGUrc","executionInfo":{"status":"ok","timestamp":1683226240311,"user_tz":-330,"elapsed":1885,"user":{"displayName":"Varma Rahul Kailash","userId":"15106114606021576928"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["### Reading excel file"],"metadata":{"id":"w4FmfdF3ni-5"}},{"cell_type":"code","source":["# Read in the Excel file\n","df = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/corpo_announcements.xlsx')"],"metadata":{"id":"XRZKRCuQGUna","executionInfo":{"status":"ok","timestamp":1683226286031,"user_tz":-330,"elapsed":26297,"user":{"displayName":"Varma Rahul Kailash","userId":"15106114606021576928"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### Regular expression"],"metadata":{"id":"waRpmwPpno4V"}},{"cell_type":"code","source":["# Define regular expression patterns for transcript and audio links\n","transcript = re.compile(r'https?://.*\\.pdf')\n","audio= re.compile(r'https?://.*\\.mp3')"],"metadata":{"id":"nlkA8rsEGUlc","executionInfo":{"status":"ok","timestamp":1683226394767,"user_tz":-330,"elapsed":417,"user":{"displayName":"Varma Rahul Kailash","userId":"15106114606021576928"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Create new columns for transcript and audio links\n","df['Transcript Link'] = ''\n","df['Audio Link'] = ''"],"metadata":{"id":"7T-lASQXGUim","executionInfo":{"status":"ok","timestamp":1683226397476,"user_tz":-330,"elapsed":397,"user":{"displayName":"Varma Rahul Kailash","userId":"15106114606021576928"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Loop through each row of the DataFrame\n","for i, row in df.iterrows():\n","    # Extract the relevant columns\n","    date = row['HEADLINE']\n","    url = row['SOURCE']\n","    desc = row['NEWSSUB']\n","    info = row['MORE']\n","    \n","    # Check if the transcript link is in the URL column\n","    if transcript.search(url):\n","        df.at[i, 'Transcript Link'] = url\n","    else:\n","        # If not, scrape the URL for the transcript link\n","        try:\n","            r = requests.get(url)\n","            soup = BeautifulSoup(r.content, 'html.parser')\n","            links = soup.find_all('a')\n","            for link in links:\n","                href = link.get('href')\n","                if transcript.search(href):\n","                    df.at[i, 'Transcript Link'] = href\n","                    break\n","        except:\n","            pass\n","    \n","    # Check if the audio link is in the description or info column\n","    if audio.search(desc):\n","        df.at[i, 'Audio Link'] = audio.search(desc).group()\n","    elif audio.search(str(info)):\n","        df.at[i, 'Audio Link'] = audio.search(info).group()"],"metadata":{"id":"-EMb9FP1GUgh","executionInfo":{"status":"ok","timestamp":1683226509708,"user_tz":-330,"elapsed":31941,"user":{"displayName":"Varma Rahul Kailash","userId":"15106114606021576928"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### Final new excel sheet"],"metadata":{"id":"ovfSrVW7nxAV"}},{"cell_type":"code","source":["# Output the new Excel file with the relevant data and links\n","df.to_excel('earnings_calls.xlsx', index=False)"],"metadata":{"id":"KW9vM9eiGUdw","executionInfo":{"status":"ok","timestamp":1683226548198,"user_tz":-330,"elapsed":26901,"user":{"displayName":"Varma Rahul Kailash","userId":"15106114606021576928"}}},"execution_count":7,"outputs":[]}]}